{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from collections import Counter\n",
    "#from config import *\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API\n",
    "from tweepy import Cursor\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import re\n",
    "import math\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                text   user  \\\n0  [hey, houston, #washyourhand, #stayhomestaysaf...  33019   \n1                  [told, grey, need, remind, covid]  81446   \n2  [recent, report, show, #hospit, across, countr...  43087   \n3  [shout, fellow, covid, longterm, survivor, wee...   1035   \n4  [moment, strength, bout, covid, use, write, fa...   1035   \n\n                                                 url  \\\n0  https://twitter.com/MTrujil35563249/status/133...   \n1  https://twitter.com/kaixoxokai/status/13302999...   \n2  https://twitter.com/RaveMSafety/status/1330299...   \n3  https://twitter.com/6MonstersMake8/status/1330...   \n4  https://twitter.com/6MonstersMake8/status/1330...   \n\n                                       original_text  \\\n0  Hey, Houston #WashYourHands #StayHomeStaySafe ...   \n1   Who told Greys we needed more reminders of covid   \n2  Recent reports show #hospitals across the coun...   \n3  Shout out to my fellow Covid long-term survivo...   \n4  I had a moment of strength during my bout of C...   \n\n                             date  \\\n0  Sat Nov 21 23:59:59 +0000 2020   \n1  Sat Nov 21 23:59:59 +0000 2020   \n2  Sat Nov 21 23:59:59 +0000 2020   \n3  Sat Nov 21 23:59:59 +0000 2020   \n4  Sat Nov 21 23:59:58 +0000 2020   \n\n                                            hashtags  likes  retweets  Cluster  \n0  [{'text': 'WashYourHands', 'indices': [13, 27]...      0         0        0  \n1                                                 []      4         0        0  \n2       [{'text': 'hospitals', 'indices': [20, 30]}]      1         0        5  \n3                                                 []      0         0        0  \n4                                                 []      0         0        0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>user</th>\n      <th>url</th>\n      <th>original_text</th>\n      <th>date</th>\n      <th>hashtags</th>\n      <th>likes</th>\n      <th>retweets</th>\n      <th>Cluster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[hey, houston, #washyourhand, #stayhomestaysaf...</td>\n      <td>33019</td>\n      <td>https://twitter.com/MTrujil35563249/status/133...</td>\n      <td>Hey, Houston #WashYourHands #StayHomeStaySafe ...</td>\n      <td>Sat Nov 21 23:59:59 +0000 2020</td>\n      <td>[{'text': 'WashYourHands', 'indices': [13, 27]...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[told, grey, need, remind, covid]</td>\n      <td>81446</td>\n      <td>https://twitter.com/kaixoxokai/status/13302999...</td>\n      <td>Who told Greys we needed more reminders of covid</td>\n      <td>Sat Nov 21 23:59:59 +0000 2020</td>\n      <td>[]</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[recent, report, show, #hospit, across, countr...</td>\n      <td>43087</td>\n      <td>https://twitter.com/RaveMSafety/status/1330299...</td>\n      <td>Recent reports show #hospitals across the coun...</td>\n      <td>Sat Nov 21 23:59:59 +0000 2020</td>\n      <td>[{'text': 'hospitals', 'indices': [20, 30]}]</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[shout, fellow, covid, longterm, survivor, wee...</td>\n      <td>1035</td>\n      <td>https://twitter.com/6MonstersMake8/status/1330...</td>\n      <td>Shout out to my fellow Covid long-term survivo...</td>\n      <td>Sat Nov 21 23:59:59 +0000 2020</td>\n      <td>[]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[moment, strength, bout, covid, use, write, fa...</td>\n      <td>1035</td>\n      <td>https://twitter.com/6MonstersMake8/status/1330...</td>\n      <td>I had a moment of strength during my bout of C...</td>\n      <td>Sat Nov 21 23:59:58 +0000 2020</td>\n      <td>[]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "data=pd.read_csv('clustering.csv')\n",
    "data.text=data.text.apply(lambda x: ast.literal_eval(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dict_to_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        aux = file.read()\n",
    "    return json.loads(aux)\n",
    "\n",
    "# Read index, tf and idf from files\n",
    "index = read_dict_to_file('index.json')\n",
    "tf = read_dict_to_file('tf.json')\n",
    "idf =read_dict_to_file('idf.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation.replace('#','').replace('@','')+'…'\n",
    "def preprocess_tweet(tweets_series):\n",
    "    # Lowercasing text\n",
    "    tweets_series = tweets_series.apply(lambda x: x.lower())\n",
    "\n",
    "    # Remove URLS (https and www), mentions and rt\n",
    "    tweets_series = tweets_series.apply(lambda x: re.sub(r'https?//\\S+|www.\\S+|@\\w*|^rt','', x))\n",
    "    \n",
    "    #Removing numbers\n",
    "    tweets_series=tweets_series.apply(lambda x: re.sub(r\"([0-9])\",'', x))\n",
    "\n",
    "    # Remove punctuation except # (hashtags)\n",
    "    tweets_series = tweets_series.apply(lambda x: \"\".join([char for char in x if char not in punctuation]))\n",
    "\n",
    "    # Replacing symbol ’ for ' as they mean the same and it is needed to correctly remove stopwords\n",
    "    tweets_series = tweets_series.apply(lambda x: x.replace(\"’\",\"\").replace('“',\"\").replace('”',''))\n",
    "    \n",
    "    #Removing emojis\n",
    "    tweets_series=tweets_series.apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "\n",
    "    # Tokenize text \n",
    "    tweets_series = tweets_series.apply(lambda x: x.split())\n",
    "\n",
    "    # Removing stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    tweets_series = tweets_series.apply(lambda x: [word for word in x if word not in stop_words])\n",
    "                \n",
    "    # Stemming\n",
    "    porter = PorterStemmer()\n",
    "    tweets_series = tweets_series.apply(lambda x: [porter.stem(word) for word in x]) \n",
    "\n",
    "    return tweets_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine Build "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "8"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankDocuments(query_terms, docs, index, idf, tf, method='tf-idf'):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    query_terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "    global data\n",
    "    df = data.iloc[docs,:]    \n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaing elements would became 0 when multiplied to the queryVector\n",
    "    docVectors=defaultdict(lambda: [0]*len(query_terms)) # I call docVectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    queryVector=[0]*len(query_terms)    \n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(query_terms) # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    \n",
    "    # HINT: use when computing tf for queryVector   \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(query_terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ## Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term] \n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [docIndex, (doc, postings)]\n",
    "            # 0 ([26, 2],[34,1])\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] #* idf[term]  # TODO: check if multiply for idf\n",
    "                \n",
    "    # calculate the score of each doc\n",
    "    # compute the cosine similarity between queryVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine siilarity\n",
    "    # see np.dot\n",
    "    \n",
    "    if method == 'popular':\n",
    "        docScores=[ [0.4*np.dot(curDocVec, queryVector) + 0.3*np.log(data['likes'].loc[doc] + 1)/np.log(data['likes'].max() + 1) +0.3*np.log(data.retweets.loc[doc] + 1)/np.log(data.retweets.max() + 1) ,doc] for doc, curDocVec in docVectors.items() ]\n",
    "        \n",
    "    elif (method == 'disparity'):\n",
    "        docScores=[ [0.4*np.dot(curDocVec, queryVector) + 0.3*np.log(data['likes'].loc[doc] + 1)/np.log(data['likes'].max() + 1) +0.3*np.log(data.retweets.loc[doc] + 1)/np.log(data.retweets.max() + 1) - (df.Cluster.value_counts()/df.Cluster.value_counts().sum())[df.loc[doc,'Cluster']]*0.356 ,doc] for doc, curDocVec in docVectors.items() ]\n",
    "        \n",
    "    else:\n",
    "        docScores=[ [np.dot(curDocVec, queryVector),doc] for doc, curDocVec in docVectors.items() ]\n",
    "    \n",
    "   \n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[x[1] for x in docScores]\n",
    "    \n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\\n\")\n",
    "        query = input(\"Insert your query:\")\n",
    "        resultDocs = search_tf_idf(query, index, method)    \n",
    "    #print ('\\n'.join(resultDocs), '\\n')\n",
    "    \n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index, method):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=preprocess_tweet(pd.Series(query)).values[0]\n",
    "\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # Set containing docs with all query terms\n",
    "            if len(docs) == 0:\n",
    "                docs = docs.union(termDocs) \n",
    "            else:\n",
    "                docs = docs.intersection(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocuments(query, docs, index, idf, tf, method)   \n",
    "\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "No results found, try again\n\nNo results found, try again\n\n=======================================\n\nTweet ID: 17678   URL: https://twitter.com/DJImmekus/status/1331743463440728064\nDate: Wed Nov 25 23:35:58 +0000 2020\nUser: 12849   Hashtags: []\nCluster: -1   Retweets: 0   Likes: 1\nTweet: Government helping with Covid lockdowns ...... https//t.co/qZpvvbz7LD\n\n=======================================\n\nTweet ID: 15537   URL: https://twitter.com/alaubsch/status/1331745092323995648\nDate: Wed Nov 25 23:42:27 +0000 2020\nUser: 59492   Hashtags: []\nCluster: 11   Retweets: 0   Likes: 1\nTweet: Lockdowns vs. COVID19 Covid Wins https//t.co/Lx5kZhraJ8\n\n=======================================\n\nTweet ID: 9396   URL: https://twitter.com/dennisclang/status/1330293553860370440\nDate: Sat Nov 21 23:34:33 +0000 2020\nUser: 69475   Hashtags: []\nCluster: 6   Retweets: 0   Likes: 0\nTweet: @91491 @15352 The PCR testing was just after the end of lockdowns, anyone who had had COVID was likely t… https//t.co/ACYHGcSOxx\n\n=======================================\n\nTweet ID: 3267   URL: https://twitter.com/MRSS11224611/status/1330297654014832641\nDate: Sat Nov 21 23:50:51 +0000 2020\nUser: 32925   Hashtags: []\nCluster: 8   Retweets: 0   Likes: 1\nTweet: @12816 @13176 Where's the proof that lockdowns have had ANY effect on transmission of covid at all? Ther… https//t.co/roVj1ytquu\n\n=======================================\n\nTweet ID: 15815   URL: https://twitter.com/merja_rantala/status/1331744870084579328\nDate: Wed Nov 25 23:41:34 +0000 2020\nUser: 87832   Hashtags: []\nCluster: -1   Retweets: 3   Likes: 19\nTweet: 😥 Unicef warns lockdown could kill more than Covid-19 as model predicts 1.2 million child deaths https//t.co/yqj6iujAc3\n\n=======================================\n\nTweet ID: 33126   URL: https://twitter.com/SimmonsBart/status/1331017266255835147\nDate: Mon Nov 23 23:30:20 +0000 2020\nUser: 47892   Hashtags: [{'text': 'COVID', 'indices': [0, 6]}]\nCluster: -1   Retweets: 4   Likes: 13\nTweet: #COVID is spread respiratorily yet COVID is a diet related disease. Of course there are outliers. Lockdowns, social… https//t.co/v0mssWrp16\n\n=======================================\n\nTweet ID: 25627   URL: https://twitter.com/MelissaSweetDr/status/1331022226330566657\nDate: Mon Nov 23 23:50:02 +0000 2020\nUser: 34998   Hashtags: []\nCluster: -1   Retweets: 2   Likes: 2\nTweet: Climate crisis CO2 hits new record despite Covid-19 lockdowns ⁦@CroakeyNews⁩ https//t.co/ewLuDWS0iu\n\n=======================================\n\nTweet ID: 11796   URL: https://twitter.com/Flobga/status/1331747983138942976\nDate: Wed Nov 25 23:53:56 +0000 2020\nUser: 18595   Hashtags: []\nCluster: 8   Retweets: 1   Likes: 2\nTweet: Lockdowns, masks &amp; vaccines Ben Swann's Covid podcast https//t.co/UinvQnAcQF via @56783\n\n=======================================\n\nTweet ID: 9357   URL: https://twitter.com/LakerJoe2/status/1330293586760523776\nDate: Sat Nov 21 23:34:41 +0000 2020\nUser: 30298   Hashtags: []\nCluster: 8   Retweets: 3   Likes: 14\nTweet: Lockdown begins tonight in the State of California Stay safe y’all wear a mask 😷 to prevent the spread of COVID 🙏🏽🙌🏽👍🏽\n\n=======================================\n\nTweet ID: 5192   URL: https://twitter.com/dutty127/status/1330296380389289991\nDate: Sat Nov 21 23:45:47 +0000 2020\nUser: 70959   Hashtags: []\nCluster: 8   Retweets: 0   Likes: 1\nTweet: Hey @78039 why did you lock down my account for replying to @26023 that if he ignored Covid rules and lockdowns… https//t.co/pjie1Q1iHZ\n\n=======================================\n\n"
    }
   ],
   "source": [
    "query = input(\"Insert your query:\")\n",
    "ranked_docs = search_tf_idf(query, index, method='disparity')    \n",
    "top = 10\n",
    "results_df = data.loc[ranked_docs[:top]][['original_text','url','user','date','hashtags','retweets','Cluster','likes']]\n",
    "\n",
    "print('=======================================\\n')\n",
    "for ind, row in results_df.iterrows():\n",
    "    print('Tweet ID:',ind,'  URL:',row.url)\n",
    "    print('Date:',row.date)\n",
    "    print('User:',row.user,'  Hashtags:',row.hashtags)\n",
    "    print('Cluster:',row.Cluster,'  Retweets:',row.retweets,'  Likes:',row.likes)\n",
    "    print('Tweet:',row.original_text)\n",
    "    print('\\n=======================================\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_list=[\"trump\",\"quarantine\",\"covid lockdown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('with_diversification_queries.tsv','w', encoding='utf-8') as f:\n",
    "    for query in queries_list:\n",
    "        # Using tf-idf\n",
    "        ranked_docs = search_tf_idf(query, index, method='disparity')    \n",
    "        top = 20\n",
    "        results_df = data.loc[ranked_docs[:top]][['original_text','url','user','date','hashtags','likes','retweets','Cluster']]\n",
    "        f.write('Query:'+query+'\\n')\n",
    "        f.write('=======================================\\n')\n",
    "        for ind, row in results_df.iterrows():\n",
    "            f.write('Tweet ID: '+str(ind)+'  URL: '+str(row.url)+'\\n')\n",
    "            f.write('Date: '+str(row.date)+'\\n')\n",
    "            f.write('User: '+str(row.user)+'  Hashtags: '+str(row.hashtags)+'\\n')\n",
    "            f.write('Likes: '+str(row.likes)+'  Retweets: '+str(row.retweets)+'  Cluster: '+str(row.Cluster)+'\\n')\n",
    "            f.write('Tweet: '+str(row.original_text)+'\\n')\n",
    "            f.write('=======================================\\n')\n",
    "        f.writelines('\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('without_diversification_queries.tsv','w', encoding='utf-8') as f:\n",
    "    for query in queries_list:\n",
    "        # Using tf-idf\n",
    "        ranked_docs = search_tf_idf(query, index, method='popular')    \n",
    "        top = 20\n",
    "        results_df = data.loc[ranked_docs[:top]][['original_text','url','user','date','hashtags','likes','retweets','Cluster']]\n",
    "        f.write('Query:'+query+'\\n')\n",
    "        f.write('=======================================\\n')\n",
    "        for ind, row in results_df.iterrows():\n",
    "            f.write('Tweet ID: '+str(ind)+'  URL: '+str(row.url)+'\\n')\n",
    "            f.write('Date: '+str(row.date)+'\\n')\n",
    "            f.write('User: '+str(row.user)+'  Hashtags: '+str(row.hashtags)+'\\n')\n",
    "            f.write('Likes: '+str(row.likes)+'  Retweets: '+str(row.retweets)+'  Cluster: '+str(row.Cluster)+'\\n')\n",
    "            f.write('Tweet: '+str(row.original_text)+'\\n')\n",
    "            f.write('=======================================\\n')\n",
    "        f.writelines('\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=======================================\n\n=======================================\n\n=======================================\n\n"
    }
   ],
   "source": [
    "with_disparity = []\n",
    "for query in queries_list:\n",
    "    # Using tf-idf\n",
    "    ranked_docs = search_tf_idf(query, index, method='disparity')    \n",
    "    top = 20\n",
    "    results_df = data.loc[ranked_docs[:top]][['original_text','url','user','date','hashtags','likes','retweets','Cluster']]\n",
    "    print('=======================================\\n')\n",
    "    for ind, row in results_df.iterrows():\n",
    "        with_disparity.append(row.Cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=======================================\n\n=======================================\n\n=======================================\n\n"
    }
   ],
   "source": [
    "without_disparity = []\n",
    "for query in queries_list:\n",
    "    # Using tf-idf\n",
    "    ranked_docs = search_tf_idf(query, index, method='popular')    \n",
    "    top = 20\n",
    "    results_df = data.loc[ranked_docs[:top]][['original_text','url','user','date','hashtags','likes','retweets','Cluster']]\n",
    "    print('=======================================\\n')\n",
    "    for ind, row in results_df.iterrows():\n",
    "        without_disparity.append(row.Cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq = np.repeat(queries_list,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_list = pd.DataFrame(\n",
    "    {'with disparity': with_disparity,\n",
    "     'without disparity': without_disparity},index=qq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = percentile_list[percentile_list.index==queries_list[0]]\n",
    "Q2 = percentile_list[percentile_list.index==queries_list[1]]\n",
    "Q3 = percentile_list[percentile_list.index==queries_list[2]]"
   ]
  },
  {
   "source": [
    "## Query 1 results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       with disparity  without disparity\ntrump               6                  0\ntrump              -1                  0\ntrump               4                  0\ntrump               4                  0\ntrump              12                  0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>with disparity</th>\n      <th>without disparity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>trump</th>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>trump</th>\n      <td>-1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>trump</th>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>trump</th>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>trump</th>\n      <td>12</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 264
    }
   ],
   "source": [
    "Q1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 1.       , -0.1650041],\n       [-0.1650041,  1.       ]])"
     },
     "metadata": {},
     "execution_count": 254
    }
   ],
   "source": [
    "np.corrcoef(Q1.values.T)            #Ranking Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[5, 4]"
     },
     "metadata": {},
     "execution_count": 265
    }
   ],
   "source": [
    "[len(Q1[x].unique()) for x in Q1.columns]      #Coverage"
   ]
  },
  {
   "source": [
    "## Query 2 results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            with disparity  without disparity\nquarantine              10                 10\nquarantine              10                 10\nquarantine              10                 10\nquarantine              10                 10\nquarantine              10                 10\nquarantine              10                 10\nquarantine              10                 10\nquarantine              10                 10\nquarantine               3                 10\nquarantine              -1                 10",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>with disparity</th>\n      <th>without disparity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>quarantine</th>\n      <td>10</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>quarantine</th>\n      <td>10</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>quarantine</th>\n      <td>10</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>quarantine</th>\n      <td>10</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>quarantine</th>\n      <td>10</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>quarantine</th>\n      <td>10</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>quarantine</th>\n      <td>10</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>quarantine</th>\n      <td>10</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>quarantine</th>\n      <td>3</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>quarantine</th>\n      <td>-1</td>\n      <td>10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 256
    }
   ],
   "source": [
    "Q2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1.        , 0.28634707],\n       [0.28634707, 1.        ]])"
     },
     "metadata": {},
     "execution_count": 257
    }
   ],
   "source": [
    "np.corrcoef(Q2.values.T)            #Ranking Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[3, 2]"
     },
     "metadata": {},
     "execution_count": 258
    }
   ],
   "source": [
    "[ len(Q2[x].unique()) for x in Q2.columns]      #Coverage"
   ]
  },
  {
   "source": [
    "## Query 3 results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                with disparity  without disparity\ncovid lockdown              -1                  0\ncovid lockdown              11                  0\ncovid lockdown               6                  0\ncovid lockdown               8                  0\ncovid lockdown              -1                 -1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>with disparity</th>\n      <th>without disparity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>covid lockdown</th>\n      <td>-1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>covid lockdown</th>\n      <td>11</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>covid lockdown</th>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>covid lockdown</th>\n      <td>8</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>covid lockdown</th>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 263
    }
   ],
   "source": [
    "Q3.head()                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1.        , 0.21692101],\n       [0.21692101, 1.        ]])"
     },
     "metadata": {},
     "execution_count": 260
    }
   ],
   "source": [
    "np.corrcoef(Q3.values.T)                    #Ranking Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[7, 2]"
     },
     "metadata": {},
     "execution_count": 261
    }
   ],
   "source": [
    "[ len(Q3[x].unique()) for x in Q3.columns]   #Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}