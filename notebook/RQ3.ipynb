{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Analysis\n",
    "---\n",
    "This notebook creates a retweet graph and runs different algorithms to predict links between a sample of nodes. \n",
    "\n",
    "The algorithms used to predict the links are Personalized Pagerank, Adamic-Adar, ALS and Jaccard Similarity. \n",
    "\n",
    "All of them are compared in terms of Accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import igraph as ig\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools \n",
    "import implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retweets = pd.read_csv(\"retweets.csv\")\n",
    "df_graph = pd.DataFrame(columns=[\"source\", \"destination\"])\n",
    "df_retweets.dropna(inplace=True)\n",
    "\n",
    "# add source-nodes\n",
    "df_graph[\"source\"] = df_retweets['user']\n",
    "\n",
    "# add destination-nodes\n",
    "df_graph[\"destination\"] = df_retweets['original_text'].apply(lambda x: x.split()[1][1:])\n",
    "df_graph.drop_duplicates(inplace=True)\n",
    "df_graph.head()\n",
    "\n",
    "df_graph.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Free memory\n",
    "del df_retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = [tuple(x) for x in df_graph.values]\n",
    "graph = ig.Graph.TupleList(tuples, directed = False, edge_attrs = ['weight'])\n",
    "original_graph = graph.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraction of edges to select as test-set\n",
    "test_size = 0.2\n",
    "\n",
    "# graph size\n",
    "N = len(graph.es)\n",
    "\n",
    "# idxs of all the edges\n",
    "all_idxs = range(N)\n",
    "\n",
    "# sample idxs of edges through the function \"choice\"\n",
    "T = np.random.choice(a=all_idxs, size=int(test_size*N), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove test_idxs from graph\n",
    "graph.delete_edges(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute groundtruth (U) and training set\n",
    "U = set() #--> set that contain the real values\n",
    "trainset = set() #--> to predict values\n",
    "for idx, one_edge in enumerate(original_graph.es):\n",
    "    \n",
    "    # take n1 and n2 idx from one_edge, that is an igraph edge *object*\n",
    "    n1 = one_edge.source\n",
    "    n2 = one_edge.target\n",
    "\n",
    "    if idx in T:\n",
    "        U.add((n1, n2, 1))\n",
    "    else:\n",
    "        trainset.add((n1, n2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17221"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nodes_idxs = set()\n",
    "\n",
    "# Obtain test nodes ids\n",
    "for n1, n2, edge in U:\n",
    "    test_nodes_idxs.add(n1)\n",
    "    test_nodes_idxs.add(n2)\n",
    "\n",
    "len(test_nodes_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nodes_at_distance_2(graph, test_nodes, num_neighbors):\n",
    "    \"\"\"\n",
    "    starting from a graph this function returns all the nodes at distance 2 from the test nodes\n",
    "    \"\"\"\n",
    "    \n",
    "    all_potential_recommendations = set()\n",
    "    for n1 in test_nodes:   \n",
    "        \n",
    "        # all the nodes at distance 1\n",
    "        nodes_at_most_distant_1 = set(graph.neighborhood(vertices=n1, order=1)) # --> Node Ids at distance 1 from node n1\n",
    "\n",
    "        # all the nodes at distance 1 and distance 2\n",
    "        nodes_at_most_distant_2 = set(graph.neighborhood(vertices=n1, order=2))\n",
    "        \n",
    "        # only the nodes at distance 2\n",
    "        only_nodes_at_distance_2 = nodes_at_most_distant_2 - nodes_at_most_distant_1\n",
    "        \n",
    "        # check if empty set\n",
    "        if len(only_nodes_at_distance_2) > 0:\n",
    "\n",
    "            for n2 in only_nodes_at_distance_2:\n",
    "                \n",
    "                # n1 is an ID of an igraph vertex\n",
    "                n1_index = n1.index \n",
    "                \n",
    "                if len(all_potential_recommendations) == num_neighbors:\n",
    "                    return all_potential_recommendations\n",
    "                \n",
    "                all_potential_recommendations.add((n1_index, n2)) \n",
    "\n",
    "    return all_potential_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neighbours at distance 2 of the test set nodes: 13602\n"
     ]
    }
   ],
   "source": [
    "# Find len(T) nodes at distance 2 from some node of the test set\n",
    "D2 = find_nodes_at_distance_2(graph,graph.vs[test_nodes_idxs], num_neighbors = len(T))\n",
    "\n",
    "# Number of neighbours of source test nodes at distance 2 from all the graph\n",
    "print('Number of neighbours at distance 2 of the test set nodes:',len(D2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add potential nodes to ground truth\n",
    "for rec in D2:\n",
    "    n1 = rec[0]\n",
    "    n2 = rec[1]\n",
    "    \n",
    "    U.add((n1,n2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_ground_truth = set()\n",
    "test_nodes_idxs = list(test_nodes_idxs)\n",
    "for n1, n2, val in U:\n",
    "    if n1 in test_nodes_idxs or n2 in test_nodes_idxs:\n",
    "        test_sample_ground_truth.add((n1, n2, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomend for each node in list_nodes the topK node (IDs) sorted by personalized pagerank\n",
    "def personalized_pagerank(graph, node, k):\n",
    "    \n",
    "    # Only keep pagerank values of other nodes\n",
    "    pr = [i for i in enumerate(graph.personalized_pagerank(reset_vertices = node)) if i[0] != node]\n",
    "    \n",
    "    # Sort by decreasing pagerank\n",
    "    out = sorted(pr, key=lambda tup: tup[1], reverse=True)[:k]\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "# Store pageranks of nodes in U in a dictionary\n",
    "PR_dict = {}\n",
    "for n1, _, _ in U:  \n",
    "    if n1 not in PR_dict.keys():\n",
    "        PR_dict[n1] = personalized_pagerank(graph, n1, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PR = pd.DataFrame(columns=['n1','n2','edge','pred_PR'])\n",
    "\n",
    "# Compute pagerank for each pair of nodes of U\n",
    "for n1, n2, edge in U:  \n",
    "    pred_PR_list = [i[1] for i in PR_dict[n1] if i[0] == n2]\n",
    "    \n",
    "    # If list empty pagerank values is 0\n",
    "    if len(pred_PR_list) != 0:\n",
    "        pred_PR = pred_PR_list[0]\n",
    "    else:\n",
    "        pred_PR = 0\n",
    "    df_PR = df_PR.append({'n1': n1, 'n2': n2, 'edge': edge,'pred_PR': pred_PR}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PR.pred_PR = df_PR.pred_PR.apply(lambda x: round(x))\n",
    "\n",
    "PR_accuracy = len(df_PR[df_PR['pred_PR'] == df_PR['edge']])/len(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adamic-Adar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ADA(u,v, graph):\n",
    "    \"\"\"\n",
    "    compute adamic-adar from scratch\n",
    "    \"\"\"  \n",
    "    # set of neighbors of u\n",
    "    outlinks_from_u = graph.neighbors(u)\n",
    "\n",
    "    # set of neighbors of v\n",
    "    inlinks_to_v = graph.neighbors(v)\n",
    "\n",
    "    # set Z of neighbors of both --> set Z = intersection of neighbors(nodes)\n",
    "    bridges = set(outlinks_from_u).intersection(inlinks_to_v)\n",
    "\n",
    "    # degree of nodes in set Z\n",
    "    deg_ = [graph.degree(n) for n in bridges] #--> looping over all the nodes of the set bridge\n",
    "    \n",
    "    # computing the reciprocal in log-scale\n",
    "    out = [1./np.log2(dd+1) for dd in deg_]\n",
    "\n",
    "    return sum(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ADA = pd.DataFrame(columns=['n1','n2','edge','pred_ADA'])\n",
    "\n",
    "# Compute Adamic-Adar between all pairs of nodes of U\n",
    "for n1, n2, edge in U:  \n",
    "    df_ADA = df_ADA.append({'n1': n1, 'n2': n2, 'edge': edge,'pred_ADA': compute_ADA(n1, n2, graph)}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ADA.pred_ADA = df_ADA.pred_ADA.apply(lambda x: round(x))\n",
    "\n",
    "ADA_accuracy = len(df_ADA[df_ADA['pred_ADA'] == df_ADA['edge']])/len(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Jaccard(u,v, graph):\n",
    "    \"\"\"\n",
    "    compute jaccard similarity\n",
    "    \"\"\"\n",
    "    # set of neighbors of u\n",
    "    outlinks_from_u = graph.neighbors(u)\n",
    "\n",
    "    # set of neighbors of v\n",
    "    inlinks_to_v =graph.neighbors(v)\n",
    "\n",
    "    # intesection of the two sets\n",
    "    num = set(outlinks_from_u).intersection(inlinks_to_v)\n",
    "    \n",
    "    # union of the two sets\n",
    "    den = set(outlinks_from_u).union(inlinks_to_v)\n",
    "    \n",
    "    if len(den) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # final division\n",
    "    out = len(num)/len(den)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Jaccard = pd.DataFrame(columns=['n1','n2','edge','pred_Jaccard'])\n",
    "\n",
    "# Compute Jaccard similarity between all pairs of nodes of U\n",
    "for n1, n2, edge in U:  \n",
    "    df_Jaccard = df_Jaccard.append({'n1': n1, 'n2': n2, 'edge': edge,'pred_Jaccard': compute_Jaccard(n1, n2, graph)},\n",
    "                                   ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Jaccard.pred_Jaccard = df_Jaccard.pred_Jaccard.apply(lambda x: round(x))\n",
    "\n",
    "#accuracy\n",
    "Jaccard_accuracy = len(df_Jaccard[df_Jaccard.edge == df_Jaccard.pred_Jaccard])/len(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89b5ca08ad44cc6a58f0eca674b101d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# first we get the adjacency matrix data\n",
    "M = graph.get_adjacency_sparse()\n",
    "\n",
    "# here we run the model ALS\n",
    "model = implicit.als.AlternatingLeastSquares(factors=250, calculate_training_loss=True,  iterations=10, random_state= 123)\n",
    "\n",
    "# train the model on a sparse matrix of item/user/confidence weights\n",
    "model.fit(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ALS(testset, model):\n",
    "    \"\"\"\n",
    "    predict for a list of observations the score for adding/removing a link\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize the empty list\n",
    "    all_predictions = []\n",
    "\n",
    "    # scroll the obs\n",
    "    for n1, n2, w in testset:\n",
    "        \n",
    "        # take here the low-dimensional vectors returned by the matrix factorization\n",
    "        \n",
    "        array_n1 = model.user_factors[n1,:]\n",
    "        array_n2 = model.item_factors[n2,:]\n",
    "\n",
    "        # multiplying these vectors we generate an approximation for the edge score\n",
    "        one_p = np.dot(array_n1, array_n2)\n",
    "\n",
    "        all_predictions.append(one_p)\n",
    "        \n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the predictions\n",
    "df_test = pd.DataFrame(list(U), columns=[\"n1\",\"n2\", \"edge\"])\n",
    "all_predictions = predict_ALS(df_test.values, model)\n",
    "\n",
    "# add predictions to df\n",
    "df_test[\"rating\"] = all_predictions\n",
    "\n",
    "# convert predictions to binary values: 0 don't add the edge, 1 add it.\n",
    "df_test[\"rating\"] = df_test[\"rating\"].apply(lambda x: int(x >= 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations matched by the prediction\n",
    "right_predictions = len(df_test[df_test[\"rating\"] == df_test[\"edge\"]])\n",
    "\n",
    "# accuracy \n",
    "ALS_accuracy = right_predictions/len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ 3A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagerank Accuracy: 0.5\n",
      "Adamic-Adar Accuracy: 0.4555212468754595\n",
      "Jaccard Accuracy: 0.4215188942802529\n",
      "ALS Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "print('Pagerank Accuracy:',PR_accuracy)\n",
    "print('Adamic-Adar Accuracy:',ADA_accuracy)\n",
    "print('Jaccard Accuracy:',Jaccard_accuracy)\n",
    "print('ALS Accuracy:',ALS_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
