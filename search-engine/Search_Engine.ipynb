{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from collections import Counter\n",
    "#from config import *\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API\n",
    "from tweepy import Cursor\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import re\n",
    "import math\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "original=pd.read_csv('originals.csv')\n",
    "original.text=original.text.apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading inverted index structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dict_to_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        aux = file.read()\n",
    "    return json.loads(aux)\n",
    "\n",
    "# Read index, tf and idf from files\n",
    "index = read_dict_to_file('index.json')\n",
    "tf = read_dict_to_file('tf.json')\n",
    "idf =read_dict_to_file('idf.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation.replace('#','')+'…'\n",
    "\n",
    "def preprocess_tweet(tweets_series):\n",
    "    # Lowercasing text\n",
    "    tweets_series = tweets_series.apply(lambda x: x.lower())\n",
    "\n",
    "    # Remove URLS (https and www), mentions and rt\n",
    "    tweets_series = tweets_series.apply(lambda x: re.sub(r'https?//\\S+|www.\\S+|@\\w*|^rt','', x))\n",
    "    \n",
    "    #Removing numbers\n",
    "    tweets_series=tweets_series.apply(lambda x: re.sub(r\"([0-9])\",'', x))\n",
    "\n",
    "    # Remove punctuation except # (hashtags)\n",
    "    tweets_series = tweets_series.apply(lambda x: \"\".join([char for char in x if char not in punctuation]))\n",
    "\n",
    "    # Replacing symbol ’ for ' as they mean the same and it is needed to correctly remove stopwords\n",
    "    tweets_series = tweets_series.apply(lambda x: x.replace(\"’\",\"\").replace('“',\"\").replace('”',''))\n",
    "    \n",
    "    #Removing emojis\n",
    "    tweets_series=tweets_series.apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "\n",
    "    # Tokenize text \n",
    "    tweets_series = tweets_series.apply(lambda x: x.split())\n",
    "\n",
    "    # Removing stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    tweets_series = tweets_series.apply(lambda x: [word for word in x if word not in stop_words])\n",
    "                \n",
    "    # Stemming\n",
    "    porter = PorterStemmer()\n",
    "    tweets_series = tweets_series.apply(lambda x: [porter.stem(word) for word in x]) \n",
    "\n",
    "    return tweets_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine Build "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankDocuments(query_terms, docs, index, idf, tf, method='tf-idf'):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based with tf-idf or popular (own score)\n",
    "    \n",
    "    Argument:\n",
    "    query_terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "    global original   \n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaing elements would became 0 when multiplied to the queryVector\n",
    "    docVectors=defaultdict(lambda: [0]*len(query_terms)) # I call docVectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    queryVector=[0]*len(query_terms)    \n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(query_terms) # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    \n",
    "    # HINT: use when computing tf for queryVector   \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(query_terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ## Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term] \n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):  \n",
    "            if doc in docs:\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex]\n",
    "                \n",
    "    # calculate the score of each doc\n",
    "    # compute the cosine similarity between queryVector and each docVector:\n",
    "    \n",
    "    # We have two methods. 'popular' is the score with our method and otherwise it computes it with tf-idf.\n",
    "    if method == 'popular':\n",
    "        # Applying formula described in the report\n",
    "        docScores=[ [0.4*np.dot(curDocVec, queryVector) + 0.3*np.log(original['likes'].loc[doc] + 1)/np.log(original['likes'].max() + 1) +0.3*np.log(original.retweets.loc[doc] + 1)/np.log(original.retweets.max() + 1) ,doc] for doc, curDocVec in docVectors.items() ]\n",
    "    else:\n",
    "        docScores=[ [np.dot(curDocVec, queryVector),doc] for doc, curDocVec in docVectors.items() ]\n",
    "        \n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[x[1] for x in docScores]\n",
    "    \n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\\n\")\n",
    "        query = input(\"Insert your query:\")\n",
    "        resultDocs = search_tf_idf(query, index, method)    \n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index, method):\n",
    "    '''\n",
    "    output is the list of documents that contain all of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the intersection of them.\n",
    "    '''\n",
    "    query=preprocess_tweet(pd.Series(query)).values[0]\n",
    "\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # Set containing docs with all query terms\n",
    "            if len(docs) == 0:\n",
    "                docs = docs.union(termDocs) \n",
    "            else:\n",
    "                docs = docs.intersection(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocuments(query, docs, index, idf, tf, method)   \n",
    "\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try some queries with metod 'popular' or 'tf-idf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:covid lockdown\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 17127   URL: https://twitter.com/twitter/statuses/1331743887040376832\n",
      "Date: Wed Nov 25 23:37:39 +0000 2020\n",
      "User: 58108   Hashtags: []\n",
      "Likes: 0   Retweets: 1\n",
      "Tweet: This is why lockdowns are worse than COVID itself https//t.co/YdCHWlOD7q\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 9910   URL: https://twitter.com/twitter/statuses/1331749477342973952\n",
      "Date: Wed Nov 25 23:59:52 +0000 2020\n",
      "User: 103048   Hashtags: []\n",
      "Likes: 1   Retweets: 0\n",
      "Tweet: \"What do think about all these Covid lockdowns?\" https//t.co/MAdHm96la3\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 29135   URL: https://twitter.com/twitter/statuses/1331019880020242439\n",
      "Date: Mon Nov 23 23:40:43 +0000 2020\n",
      "User: 39359   Hashtags: []\n",
      "Likes: 0   Retweets: 0\n",
      "Tweet: https//t.co/JIH3qoK0LJ https//t.co/efu2qUA6OU we are both on covid lockdown so tune in\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 19437   URL: https://twitter.com/twitter/statuses/1331742094507925505\n",
      "Date: Wed Nov 25 23:30:32 +0000 2020\n",
      "User: 102448   Hashtags: []\n",
      "Likes: 203   Retweets: 105\n",
      "Tweet: 90-year-old woman opts for assisted suicide when faced with the possibility of another COVID-19 lockdown https//t.co/0cDMgesJse\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 17678   URL: https://twitter.com/twitter/statuses/1331743463440728064\n",
      "Date: Wed Nov 25 23:35:58 +0000 2020\n",
      "User: 12849   Hashtags: []\n",
      "Likes: 1   Retweets: 0\n",
      "Tweet: Government helping with Covid lockdowns ...... https//t.co/qZpvvbz7LD\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 13052   URL: https://twitter.com/twitter/statuses/1331746996735119360\n",
      "Date: Wed Nov 25 23:50:01 +0000 2020\n",
      "User: 20118   Hashtags: []\n",
      "Likes: 0   Retweets: 0\n",
      "Tweet: @26329 @13126 @15762 @15720 You asked why lockdown for covid when there is no suc… https//t.co/wMjgp948Xx\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 23222   URL: https://twitter.com/twitter/statuses/1331023895495921664\n",
      "Date: Mon Nov 23 23:56:40 +0000 2020\n",
      "User: 1232   Hashtags: []\n",
      "Likes: 0   Retweets: 0\n",
      "Tweet: @79817 let’s not lockdown our province. Covid deaths are more natural than suicides and opioid overdose deaths caused by lockdowns.\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 10213   URL: https://twitter.com/twitter/statuses/1331749221100351489\n",
      "Date: Wed Nov 25 23:58:51 +0000 2020\n",
      "User: 50275   Hashtags: []\n",
      "Likes: 70   Retweets: 10\n",
      "Tweet: Small business are being crushed by the COVID lockdown. Are we surprised that they are fighting to stay open? I can… https//t.co/ONe3NV7Cl5\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 8128   URL: https://twitter.com/twitter/statuses/1330294423540936709\n",
      "Date: Sat Nov 21 23:38:00 +0000 2020\n",
      "User: 62027   Hashtags: []\n",
      "Likes: 0   Retweets: 0\n",
      "Tweet: @68735 The majority who catch Covid-19 are those lockdown at home.\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 339   URL: https://twitter.com/twitter/statuses/1330299696938684418\n",
      "Date: Sat Nov 21 23:58:58 +0000 2020\n",
      "User: 92735   Hashtags: []\n",
      "Likes: 0   Retweets: 0\n",
      "Tweet: Me thinks people are tired of Covid lockdowns! https//t.co/9BLLYwRsBK\n",
      "\n",
      "=======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Insert your query:\")\n",
    "#method accepts two options: 'popular' and 'tf-idf'\n",
    "ranked_docs = search_tf_idf(query, index, method='popular')    \n",
    "top = 10\n",
    "results_df = original.loc[ranked_docs[:top]][['original_text','url','user','date','hashtags','likes','retweets']]\n",
    "\n",
    "print('=======================================\\n')\n",
    "for ind, row in results_df.iterrows():\n",
    "    print('Tweet ID:',ind,'  URL:',row.url)\n",
    "    print('Date:',row.date)\n",
    "    print('User:',row.user,'  Hashtags:',row.hashtags)\n",
    "    print('Likes:',row.likes,'  Retweets:',row.retweets)\n",
    "    print('Tweet:',row.original_text)\n",
    "    print('\\n=======================================\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating method with Word2Vector + cosine similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Loading Word2Vector model\n",
    "w2v_model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_vec(tweet,model=w2v_model):\n",
    "    # Appending in a list the embedded representation of all the words in the tweet if they are in vocab\n",
    "    query_vectors=[w2v_model.wv.word_vec(word) for word in tweet if word in w2v_model.wv.vocab]\n",
    "    # In case this list of vectors have length grater than 0 we compute the average value\n",
    "    if len(query_vectors)>0:\n",
    "        vec=np.average(np.array(query_vectors),axis=0)\n",
    "        return vec/np.linalg.norm(vec)\n",
    "    # Otherwise we return a 0\n",
    "    return np.zeros((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same function as emb_vec but returns the list of words that were in vocabulary\n",
    "def calculate_w2v_vector(lista,model=w2v_model):\n",
    "    query_vectors=[]\n",
    "    for word in lista:\n",
    "        if word in w2v_model.wv.vocab:\n",
    "            query_vectors.append(w2v_model.wv.word_vec(word))\n",
    "        else:\n",
    "            lista.remove(word)\n",
    "    vec=np.average(np.array(query_vectors),axis=0)\n",
    "    return lista,vec/np.linalg.norm(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_docs_w2v(query_terms,query_vec, docs, index, model=w2v_model):\n",
    "    global original \n",
    "    docVectors=dict() # I call docVectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    # For each doc we calculate it's vector representation\n",
    "    for doc in docs:\n",
    "        terms=original['text'][doc]\n",
    "        _,docVectors[doc]=calculate_w2v_vector(terms,model=w2v_model)\n",
    "        \n",
    "    # Compputing doc scores as the cosine similarity between vector representation of query and doc\n",
    "    docScores=[ [np.dot(curDocVec, query_vec),doc] for doc, curDocVec in docVectors.items() ]\n",
    "    # Sorting the results\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[x[1] for x in docScores]\n",
    "    \n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\\n\")\n",
    "        query = input(\"Insert your query:\")\n",
    "        resultDocs = compute_w2v_rank(query, index, model=w2v_model)    \n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_w2v_rank(query, index, model=w2v_model):\n",
    "    '''\n",
    "    output is the list of documents that contain all of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the intersection of them.\n",
    "    '''\n",
    "    query=preprocess_tweet(pd.Series(query)).values[0]\n",
    "    lista,query_vec=calculate_w2v_vector(query,model=w2v_model)\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            # Set containing docs with all query terms\n",
    "            if len(docs) == 0:\n",
    "                docs = docs.union(termDocs) \n",
    "            else:\n",
    "                docs = docs.intersection(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rank_docs_w2v(lista,query_vec, docs, index, model=w2v_model)   \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:covid lockdown\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 29135   URL: https://twitter.com/twitter/statuses/1331019880020242439\n",
      "Date: Mon Nov 23 23:40:43 +0000 2020\n",
      "User: 39359   Hashtags: []\n",
      "Likes: 0   Retweets: 0\n",
      "Tweet: https//t.co/JIH3qoK0LJ https//t.co/efu2qUA6OU we are both on covid lockdown so tune in\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 17127   URL: https://twitter.com/twitter/statuses/1331743887040376832\n",
      "Date: Wed Nov 25 23:37:39 +0000 2020\n",
      "User: 58108   Hashtags: []\n",
      "Likes: 0   Retweets: 1\n",
      "Tweet: This is why lockdowns are worse than COVID itself https//t.co/YdCHWlOD7q\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 13052   URL: https://twitter.com/twitter/statuses/1331746996735119360\n",
      "Date: Wed Nov 25 23:50:01 +0000 2020\n",
      "User: 20118   Hashtags: []\n",
      "Likes: 0   Retweets: 0\n",
      "Tweet: @26329 @13126 @15762 @15720 You asked why lockdown for covid when there is no suc… https//t.co/wMjgp948Xx\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 9910   URL: https://twitter.com/twitter/statuses/1331749477342973952\n",
      "Date: Wed Nov 25 23:59:52 +0000 2020\n",
      "User: 103048   Hashtags: []\n",
      "Likes: 1   Retweets: 0\n",
      "Tweet: \"What do think about all these Covid lockdowns?\" https//t.co/MAdHm96la3\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 10440   URL: https://twitter.com/twitter/statuses/1331749013075365889\n",
      "Date: Wed Nov 25 23:58:02 +0000 2020\n",
      "User: 68254   Hashtags: []\n",
      "Likes: 0   Retweets: 0\n",
      "Tweet: What England's Covid lockdown rules mean for you - and when they could end https//t.co/8McDT990Ty… https//t.co/8F0coN25Zc\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 15537   URL: https://twitter.com/twitter/statuses/1331745092323995648\n",
      "Date: Wed Nov 25 23:42:27 +0000 2020\n",
      "User: 59492   Hashtags: []\n",
      "Likes: 1   Retweets: 0\n",
      "Tweet: Lockdowns vs. COVID19 Covid Wins https//t.co/Lx5kZhraJ8\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 8128   URL: https://twitter.com/twitter/statuses/1330294423540936709\n",
      "Date: Sat Nov 21 23:38:00 +0000 2020\n",
      "User: 62027   Hashtags: []\n",
      "Likes: 0   Retweets: 0\n",
      "Tweet: @68735 The majority who catch Covid-19 are those lockdown at home.\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 2576   URL: https://twitter.com/twitter/statuses/1330298122791215104\n",
      "Date: Sat Nov 21 23:52:42 +0000 2020\n",
      "User: 102785   Hashtags: []\n",
      "Likes: 0   Retweets: 3\n",
      "Tweet: Toronto Goes Into Another Lockdown Again As Covid-19 Rises https//t.co/ycknlvBsKz https//t.co/DL44giaN2T\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 30130   URL: https://twitter.com/twitter/statuses/1331019225540997127\n",
      "Date: Mon Nov 23 23:38:07 +0000 2020\n",
      "User: 106474   Hashtags: []\n",
      "Likes: 0   Retweets: 0\n",
      "Tweet: COVID-19 Ontario Small businesses question value of COVID-19 lockdowns in Toronto and Peel Region https//t.co/7iatMioNjZ\n",
      "\n",
      "=======================================\n",
      "\n",
      "Tweet ID: 881   URL: https://twitter.com/twitter/statuses/1330299313289916428\n",
      "Date: Sat Nov 21 23:57:26 +0000 2020\n",
      "User: 101075   Hashtags: []\n",
      "Likes: 1   Retweets: 0\n",
      "Tweet: @58175 No lockdown? this guy on crack. the covid cases are going to rise\n",
      "\n",
      "=======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Insert your query:\")\n",
    "ranked_docs = compute_w2v_rank(query, index, model=w2v_model)  \n",
    "top = 10\n",
    "\n",
    "results_df = original.loc[ranked_docs[:top]][['original_text','url','user','date','hashtags','likes','retweets']]\n",
    "\n",
    "print('=======================================\\n')\n",
    "for ind, row in results_df.iterrows():\n",
    "    print('Tweet ID:',ind,'  URL:',row.url)\n",
    "    print('Date:',row.date)\n",
    "    print('User:',row.user,'  Hashtags:',row.hashtags)\n",
    "    print('Likes:',row.likes,'  Retweets:',row.retweets)\n",
    "    print('Tweet:',row.original_text)\n",
    "    print('\\n=======================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
